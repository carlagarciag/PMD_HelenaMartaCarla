{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "caractselec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlagarciag/PMD_HelenaMartaCarla/blob/main/caractselec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sznT_d50iC2M",
        "outputId": "f89720b5-5184-48d4-d3e9-46993dde7fdc"
      },
      "source": [
        "# Installing pySpark and importing some useful packages\n",
        "!pip install pyspark[sql]\n",
        "\n",
        "from __future__ import print_function\n",
        "from functools import wraps\n",
        "import pyspark as spark\n",
        "from pyspark import SparkConf\n",
        "import time\n",
        "from operator import add\n",
        "import os \n",
        "from subprocess import STDOUT, check_call, check_output\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark[sql]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from pyspark[sql]) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pyspark[sql]) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->pyspark[sql]) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->pyspark[sql]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->pyspark[sql]) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->pyspark[sql]) (1.15.0)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=cb38589cf7250fb1247f0b1ce89c0ee3cccd69b06ba2aa4907407fb1a899ae45\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA2V22KE9U4L"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_2EOjXL9VAJ"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9JeC4Hd6i9-",
        "outputId": "c3eb5c79-e51b-47db-b64b-1468e7b99422"
      },
      "source": [
        "from pyspark import SparkFiles\n",
        "\n",
        "df = sqlContext.read.csv(SparkFiles.get(\"healthcare-dataset-stroke-data-preprocessed.csv\"), header=True, inferSchema= True)\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: double (nullable = true)\n",
            " |-- hypertension: integer (nullable = true)\n",
            " |-- heart_disease: integer (nullable = true)\n",
            " |-- ever_married: integer (nullable = true)\n",
            " |-- work_type: integer (nullable = true)\n",
            " |-- avg_glucose_level: double (nullable = true)\n",
            " |-- bmi: double (nullable = true)\n",
            " |-- smoking_status: integer (nullable = true)\n",
            " |-- stroke: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqBIddc666ZL",
        "outputId": "f02a8315-b904-492d-f010-9069dff00a10"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------+-------------+------------+---------+-----------------+----+--------------+------+\n",
            "| age|hypertension|heart_disease|ever_married|work_type|avg_glucose_level| bmi|smoking_status|stroke|\n",
            "+----+------------+-------------+------------+---------+-----------------+----+--------------+------+\n",
            "|67.0|           0|            1|           1|        2|91.88499999999999|36.6|             1|     1|\n",
            "|61.0|           0|            0|           1|        3|          91.8825|28.1|             2|     1|\n",
            "|80.0|           0|            1|           1|        2|           105.92|32.5|             2|     1|\n",
            "|49.0|           0|            0|           1|        2|           171.23|34.4|             3|     1|\n",
            "|79.0|           1|            0|           1|        3|           174.12|24.0|             2|     1|\n",
            "|81.0|           0|            0|           1|        2|           186.21|29.0|             1|     1|\n",
            "|74.0|           1|            1|           1|        2|            70.09|27.4|             2|     1|\n",
            "|69.0|           0|            0|           0|        2|            94.39|22.8|             2|     1|\n",
            "|59.0|           0|            0|           1|        2|            76.15|28.1|             0|     1|\n",
            "|78.0|           0|            0|           1|        2|            58.57|24.2|             0|     1|\n",
            "|81.0|           1|            0|           1|        2|            80.43|29.7|             2|     1|\n",
            "|61.0|           0|            1|           1|        0|           120.46|36.8|             3|     1|\n",
            "|54.0|           0|            0|           1|        2|           104.51|27.3|             3|     1|\n",
            "|78.0|           0|            1|           1|        2|         91.88125|28.1|             0|     1|\n",
            "|79.0|           0|            1|           1|        2|        91.880625|28.2|             2|     1|\n",
            "|50.0|           1|            0|           1|        3|           167.41|30.9|             2|     1|\n",
            "|64.0|           0|            1|           1|        2|           191.61|37.5|             3|     1|\n",
            "|75.0|           1|            0|           1|        2|       91.8803125|25.8|             3|     1|\n",
            "|60.0|           0|            0|           0|        2|            89.22|37.8|             2|     1|\n",
            "|57.0|           0|            1|           0|        0|      91.88015625|28.1|             0|     1|\n",
            "+----+------------+-------------+------------+---------+-----------------+----+--------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IHeu_O-oVUm",
        "outputId": "cb704b00-ca99-45c3-ec40-2d17807b3049"
      },
      "source": [
        "train, test = df.randomSplit([0.7, 0.3], seed= 12345)\n",
        "print(\"El número de pacientes para el conjunto de train es: \" + str(train.count()))\n",
        "print(\"El número de pacientes para el conjunto de test es: \" + str(test.count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El número de pacientes para el conjunto de train es: 6717\n",
            "El número de pacientes para el conjunto de test es: 2887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZW-FMpnond_"
      },
      "source": [
        "X_train = train.select('age', 'bmi', 'avg_glucose_level', 'hypertension', 'heart_disease', 'smoking_status', 'ever_married', 'work_type')\n",
        "Y_train = train.select('stroke')\n",
        "\n",
        "X_test = train.select('age', 'bmi', 'avg_glucose_level', 'hypertension', 'heart_disease', 'smoking_status', 'ever_married', 'work_type')\n",
        "Y_test = train.select('stroke')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh_X_fx02SfK",
        "outputId": "06c91484-a60a-4e22-f1c3-bd4110ab8314"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# transformer\n",
        "vector_assembler = VectorAssembler(inputCols=['age', 'bmi', 'avg_glucose_level', 'hypertension', 'heart_disease', 'smoking_status', 'ever_married', 'work_type'],outputCol=\"features\")\n",
        "df_temp = vector_assembler.transform(df)\n",
        "df_temp.show(3)\n",
        "\n",
        "# drop the original data features column\n",
        "df = df_temp.drop('age', 'bmi', 'avg_glucose_level', 'hypertension', 'heart_disease', 'smoking_status', 'ever_married', 'work_type')\n",
        "df.show(3)\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# estimator\n",
        "l_indexer = StringIndexer(inputCol=\"stroke\", outputCol=\"labelIndex\")\n",
        "df = l_indexer.fit(df).transform(df)\n",
        "df.show(3)\n",
        "# data splitting\n",
        "(training,testing) = df.randomSplit([0.7,0.3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------+-------------+------------+---------+-----------------+----+--------------+------+--------------------+\n",
            "| age|hypertension|heart_disease|ever_married|work_type|avg_glucose_level| bmi|smoking_status|stroke|            features|\n",
            "+----+------------+-------------+------------+---------+-----------------+----+--------------+------+--------------------+\n",
            "|67.0|           0|            1|           1|        2|91.88499999999999|36.6|             1|     1|[67.0,36.6,91.884...|\n",
            "|61.0|           0|            0|           1|        3|          91.8825|28.1|             2|     1|[61.0,28.1,91.882...|\n",
            "|80.0|           0|            1|           1|        2|           105.92|32.5|             2|     1|[80.0,32.5,105.92...|\n",
            "+----+------------+-------------+------------+---------+-----------------+----+--------------+------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------+--------------------+\n",
            "|stroke|            features|\n",
            "+------+--------------------+\n",
            "|     1|[67.0,36.6,91.884...|\n",
            "|     1|[61.0,28.1,91.882...|\n",
            "|     1|[80.0,32.5,105.92...|\n",
            "+------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------+--------------------+----------+\n",
            "|stroke|            features|labelIndex|\n",
            "+------+--------------------+----------+\n",
            "|     1|[67.0,36.6,91.884...|       1.0|\n",
            "|     1|[61.0,28.1,91.882...|       1.0|\n",
            "|     1|[80.0,32.5,105.92...|       1.0|\n",
            "+------+--------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAGTkd764zPn"
      },
      "source": [
        "ÁRBOL DE DECISIÓN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUtI1Udo3oxh",
        "outputId": "09389a1f-240d-44de-e6d2-c3f2bdaf8497"
      },
      "source": [
        "%%time\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# train our model using training data\n",
        "dt = DecisionTreeClassifier(labelCol=\"labelIndex\", featuresCol=\"features\")\n",
        "model = dt.fit(training)\n",
        "\n",
        "# test our model and make predictions using testing data\n",
        "predictions = model.transform(testing)\n",
        "predictions.select(\"prediction\", \"labelIndex\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+\n",
            "|prediction|labelIndex|\n",
            "+----------+----------+\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 53.9 ms, sys: 13.4 ms, total: 67.3 ms\n",
            "Wall time: 5.11 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozm24tCt31ab",
        "outputId": "c4aaaec7-6124-4ebb-bf92-b5404b75cb37"
      },
      "source": [
        "%%time\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.199106 \n",
            "Accuracy = 0.800894 \n",
            "CPU times: user 15 ms, sys: 3.25 ms, total: 18.3 ms\n",
            "Wall time: 779 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHL126GW7wAh",
        "outputId": "47e52578-1c2b-4320-e82e-09b0f56e339e"
      },
      "source": [
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "prediction_label = predictions.select(\"prediction\", \"labelIndex\")\n",
        "\n",
        "metricsp = MulticlassMetrics(prediction_label.rdd.map(tuple))\n",
        "\n",
        "matrix = metricsp.confusionMatrix()\n",
        "\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DenseMatrix([[ 997.,  454.],\n",
            "             [ 125., 1332.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj4DPpZH9Int"
      },
      "source": [
        "especificidad = 0.6871\n",
        "\n",
        "sensibilidad = 0.9142"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0iU6zkL44ih"
      },
      "source": [
        "REGRESIÓN LOGÍSTICA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7G59SuP48Es",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90b6c5a-fe36-41cd-d4fc-6655e2fcd02f"
      },
      "source": [
        "%%time\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'labelIndex', maxIter = 100)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "predictions = lrModel.transform(testing)\n",
        "\n",
        "predictions.select(\"prediction\", \"labelIndex\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+\n",
            "|prediction|labelIndex|\n",
            "+----------+----------+\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "|       0.0|       0.0|\n",
            "+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "CPU times: user 61.4 ms, sys: 5.24 ms, total: 66.6 ms\n",
            "Wall time: 4.51 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkTFKXQx48JD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156673ff-e103-477e-e1e8-7da7282c4d26"
      },
      "source": [
        "%%time\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "print(\"Accuracy = %g \" % accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.206671 \n",
            "Accuracy = 0.793329 \n",
            "CPU times: user 7.83 ms, sys: 958 µs, total: 8.78 ms\n",
            "Wall time: 458 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCMsblDf708l",
        "outputId": "f868d87d-f7c4-40c0-df00-6f933f8e3016"
      },
      "source": [
        "prediction_label = predictions.select(\"prediction\", \"labelIndex\")\n",
        "\n",
        "metricsp = MulticlassMetrics(prediction_label.rdd.map(tuple))\n",
        "\n",
        "matrix = metricsp.confusionMatrix()\n",
        "\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DenseMatrix([[1103.,  348.],\n",
            "             [ 253., 1204.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmHAFU-j9ju3"
      },
      "source": [
        "especificidad = 0.7601\n",
        "\n",
        "sensibilidad = 0.8264"
      ]
    }
  ]
}